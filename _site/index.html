<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title></title>
  <style type="text/css">
  body
  {
    margin:40px auto;
    max-width:680px;
    line-height:1.4;
    font-size:120%;
    color:#111;
    padding:0 10px;
    background-color: #FDFDFD;
  }

  a{
    text-decoration: none;
    color: cornflowerblue;
  }

  a:hover{
    text-decoration: underline;
  }

  h1,h2,h3
  {
    line-height:1.2;
  }

  article{
    margin-top:200px;
  }
  .caption{
    width:100%;
  }
  .centered{
    margin:0 auto;
    display:block;
  }

  code{
    font-size:1em;
    white-space: pre;
    background-color:ghostwhite;
    display:block;
    overflow: auto;
    padding: 0.6em 0.8em;
  }


  </style>
</head>

  
  <body>
  
  <div class="container content">
      <header class="masthead">
        <h2 class="masthead-title">
          <a href="/" title="Home">Steven LoFurno</a>
          
             
              &nbsp;&nbsp;&nbsp;
              <small><a href="/about">About</a></small>
            
              &nbsp;&nbsp;&nbsp;
              <small><a href="/">Posts</a></small>
            
              &nbsp;&nbsp;&nbsp;
              <small><a href="/projects">Projects</a></small>
            
        </h2>
        <small>a programming blog by a retired pro-gamer</small>
      </header>
      
    <main>
        
<article class="post">
<h1 class="post-title">i don't write c often but when I do it looks like go</h1>
<time datetime="2015-11-16T00:00:00-05:00" class="post-date">16 Nov 2015</time>
  <p>Libmill is a c library for working with coroutines in a style that is simliar to golang, complete with a syntactically similiar version of channels, select statements, and goroutines.</p>

<p>Nanomsg is the reference implementation of the scalability protocols, a set of communications patterns which are used for creating distributed systems.</p>

<p>While you can use them separately, together they show alot of synergy. Libmill operates in the context of a single thread, so you need to avoid blocking. To make this possible, libmill provides nonblocking versions of io functions like reading from a file or sleeping. Nanomsg provides a flag to allow for either blocking or non blocking io calls. Since we don’t want to block the thread, we can ask nanomsg for a file descripter which will signal when there is data available. We can combine this with libmill’s wait to yield our goroutine until  there is data to be read.</p>

<p>My plan is to build a simple distributed system to process movie files into gifs. Users should be able to upload movie files in common formats eg mp4 or avi, where a broker will distribute the work to the first available worker. This worker will do all the processing, and when its done, will return a gif.</p>

<p>This design is called the <a href="http://zguide.zeromq.org/page:all#advanced-request-reply">load balancing broker</a> Whenever a worker is ready to do more work, it will send a request to our broker. Instead of replying immediately, the broker will queue up this worker until it has work for it to do. The worker’s request also serves as an opportunity for the worker to return the result of its last job. When the next job comes in, our broker will get the next worker in the queue and finally send it’s reply with the movie file we wish to process.</p>

<p>I will break this system down into 3 main pieces</p>

<p>A collect loop, which gets results from the workers</p>

<ul>
  <li>receieve worker request</li>
  <li>read uuid + gif body from worker message</li>
  <li>enqueue the worker</li>
  <li>broadcasting the uuid, so the subscribed client knows their gif is ready</li>
  <li>save the gif</li>
</ul>

<p>A routing loop, which sends jobs to the workers</p>

<ul>
  <li>wait for next gif request</li>
  <li>dequeue next worker</li>
  <li>reply to worker with request uuid + movie file</li>
</ul>

<p>a http server, which gets jobs from our clients</p>

<ul>
  <li>listen for post requests</li>
  <li>receive movie file</li>
  <li>generate uuid</li>
  <li>enqueue gif request</li>
  <li>subscribe to topic uuid</li>
  <li>wait for uuid to be published</li>
</ul>

<p>before I jump into my implementation, I want to cover a few details of how nanomsg works. The key to nanomsg is the AF_SP_RAW socket. with a normal request and reply AF_SP socket setup, we are limited to a linear request-reply-request-reply pattern. In order to build any kind of interesting system, we need to not only be able to handle requests in any order, but we need to be able to save some kind of information so we can reply to the right socket at a later time. There are versions of the send and recv functions that give us access to the routing header we need, called sendmsg and recvmsg.</p>

<p>The normal send and recv just take a buffer of bytes to send as a message, but the raw versions require you to use a special header struct called nn_msghdr. The msghdr struct gives you the option to provide an array of iovec structs. I havn’t found the need for it yet, so I’ve just been using an iovec of length 1.</p>

<pre><code>struct nn_iovec{
  size_t iov_len;
  char *iov_base;
}

struct nn_msghdr{
  size_t msg_iovlen;
  struct nn_iovec msg_iov;
  size_t msg_controllen;
  char *msg_control;
}
</code></pre>

<p>Heres a quick and dirty example. Garrett is just now working on a pr to allow for internally allocated headers, so I went with a hardcoded length of 64. Nanomsg actually has a default max messagesize it can receive. Since we are dealing in movies and gifs, we can easily have filesizes in the 10’s or 100’s of mbs. Nanomsg won’t accept anything over the NN_RCVMAXSIZE, so here I have set it to -1, which limits the receive size only by our available system memory.</p>

<pre><code>int worker_router = nn_socket(AF_SP_RAW, NN_REP);

nn_bind(worker_router, WORKERROUTER);

int rcv_max = -1;
nn_setsockopt(worker_router, NN_SOL_SOCKET,
        NN_RCVMAXSIZE, &amp;rcv_max, sizeof(rcv_max));


struct nn_msghdr hdr;
memset(&amp;hdr, 0, sizeof(hdr));

char *ctrl = malloc(sizeof(char)*64);
char *body = malloc(sizeof(char)*REC_SIZE);

struct nn_iovec iovec;
iovec.iov_base = body;
iovec.iov_len = REC_SIZE;

hdr.msg_iov = &amp;iovec;
hdr.msg_iovlen = 1;
hdr.msg_control = ctrl;
hdr.msg_controllen = 64;

int rc = nn_recvmsg(worker_router, &amp;hdr, NN_DONTWAIT);

chs(workers, char*, ctrl);
</code></pre>

<p>after running this, I just stick the entire header into a channel. <b>That’s all the information we’ll need to reply to this request later on</b>. You would have to keep track of the size of the header, but for now I’m cheating so I know its always going to be 64. The length of the body data is returned from nn_recvmsg, and the actual body data is stored in our body buffer. Here REC_SIZE is a constant which gives me a buffer thats big enough to receive a message. You can let nanomsg allocate this buffer to the right size for you, passing in a null pointer and NN_MSG as the length, but im going to wait for the next pr for this.</p>

<p>For the system we’re building, the body buffer will contain either nothing (if its the first time a worker has connected), or a uuid + gif.</p>

<p>Keep in mind we are making alot of assumptions to simplify our system. We are assuming that nothing ever goes wrong… our workers will never crash and our network will never partition. With that in mind, any time we get a gif request, we can just pull the next routing header from our channel and use it to construct a reply.</p>

<p>To make things simplier, and also more golike, I made a quick structure called a slice. Just like in go it has a current len and capacity. It made my life easier when reading a large stream of bytes (ie a gif) by taking care of all the allocation and bookkeeping for me. With that in mind, here is the struct used to represent a request, or “job”.</p>

<pre><code>struct gif_request {
    slice *uuid;
    slice *data;
};
</code></pre>

<p>this is my entire routing loop. It waits for a request, waits for a worker, and then sends the worker all the information it needs to make our gif and let the client know when its ready.</p>

<pre><code>coroutine void start_router(chan workers, int worker_router, chan jobs)
{
  void *buf = NULL;

  while(1){
    //wait for the next job
    gif_request *job = chr(jobs, gif_request*);
    size_t sum = job-&gt;uuid-&gt;len-1 + job-&gt;data-&gt;len;

    //ask nanomsg for a buffer big enough for our uuid+video file
    buf = nn_allocmsg (sum, 0);
    memcpy(buf, job-&gt;uuid-&gt;bytes, 36);
    memcpy(buf+36, job-&gt;data-&gt;bytes, job-&gt;data-&gt;len);
    free_slice(job-&gt;data);

    //wait for a worker
    char *worker_header = chr(workers, char*);

    struct nn_msghdr hdr;
    memset(&amp;hdr, 0, sizeof(hdr));

    hdr.msg_control = worker_header;
    hdr.msg_controllen = 64;

    struct nn_iovec iovec;
    iovec.iov_base = &amp;buf;
    //let nanomsg know we using its zero-copy buffer
    iovec.iov_len = NN_MSG;

    hdr.msg_iov = &amp;iovec;
    hdr.msg_iovlen = 1;

    nn_sendmsg(worker_router, &amp;hdr, NN_DONTWAIT);
  }
}
</code></pre>

<p>Keep in mind that our worker makes a request to our broker. If we don’t have a high volume of gif requests, a worker might sit in this queue for a long time. Nanomsg will try to resend any request that it doesn’t get a reply from, which is not the behavior we want. The default interval is every 60 seconds. Unfortunately we can’t specify an unlimited interval, but we can set it to 2 billion milliseconds, which is about 23 days.</p>

<pre><code>int rsnd = 2000000000;
nn_setsockopt(router, NN_REQ, NN_REQ_RESEND_IVL, &amp;rsnd, sizeof(rsnd));
</code></pre>

</article>


<article class="post">
<h1 class="post-title">what a disaster (statically linking go binaries for 32 bit alpine linux)</h1>
<time datetime="2015-11-02T00:00:00-05:00" class="post-date">02 Nov 2015</time>
  <p>in progress…</p>

<p>http://alpinelinux.org/downloads/
http://wiki.alpinelinux.org/wiki/Install_to_disk
setup-alpine</p>

<p>mount -t iso9660 /dev/cdrom /media/</p>

<p>echo “/media/apks” » /etc/apk/repositories</p>

<p>/var/cache/apks
http://nl.alpinelinux.org/alpine/v3.2/main
#http://nl.alpinelinux.org/alpine/edge/main
#http://nl.alpinelinux.org/alpine/edge/testing</p>

<p>http://wiki.alpinelinux.org/wiki/Connecting_to_a_wireless_access_point</p>

<p>wpa_supplicant -Dwext -iwlan0 -c ./wpa.conf
udhcpc -i wlan0
ubuntu: iface wlan0 inet dhcp
 dhclient -i wlan0
 or something??
 /etc/ssh/sshd_config -&gt; make sure rootlogin isnt without-pass
 service ssh restart</p>

<p>go binary dist is hardcoded to use gcc…
alpine uses http://www.musl-libc.org/
go compiler doesnt work
go binaries compiled for linux dont work</p>

<p>env GOOS=linux GOARCH=386 go build -a .</p>

<p>Trace/breakpoint trap</p>

<p>echo 1 &gt; /proc/sys/kernel/modify_ldt</p>

<p>https://github.com/golang/go/search?utf8=%E2%9C%93&amp;q=modify_ldt</p>

<pre><code>When linking against the system libraries, we use its pthread_create... To insulate the rest of the tool chain from this ugliness, 8l rewrites 0(TLS) into -4(GS) for us.

// call modify_ldt
MOVL	$1, BX	// func = 1 (write)
MOVL	AX, CX	// user_desc
MOVL	$16, DX	// sizeof(user_desc)
MOVL	$123, AX	// syscall - modify_ldt
INVOKE_SYSINFO
</code></pre>

<p>wrong version of libuv…
cause package repo had main commented out</p>

<p>http://git.alpinelinux.org/cgit/aports/tree/main/nodejs/APKBUILD
http://git.alpinelinux.org/cgit/aports/tree/main/nodejs/APKBUILD?id=75cc781209536288b4522d523e2575c184312869</p>

<p>apk add python openssl-dev zlib-dev libuv-dev	linux-headers paxmark</p>

<p>./configure –shared-zlib –shared-libuv –shared-openssl</p>

<p>paxctl -cm /usr/local/bin/node</p>

</article>


<article class="post">
<h1 class="post-title">meteor-sucks.go</h1>
<time datetime="2015-10-26T00:00:00-04:00" class="post-date">26 Oct 2015</time>
  <p>Meteor seems to be pretty hot right now, and I’ve been watching some people build their first meteor apps. Even when they struggled getting things to work, people seemed genuinely enamored with meteor, so I decided to give Meteor a shot and play the role of devils advocate.</p>

<p>In general I prefer to build things out of small composable pieces. In contrast, Meteor is less of a framework and more of a way of life. Because the entire stack is javascript, it provides Meteor the opportunity to own everything from your database to your dom rendering, and everything in between. While Meteor makes it very easy to do things the Meteor way, what happens when you want to do something slightly differently?</p>

<p>At the core of Meteor is the idea of a reactive data source. This is essentially an observable collection with callback methods any time objects in the collection are added, removed, or changed. Meteor calls an object which fulfills this interface a cursor, and by publishing a cursor on the server, Meteor will automagically forward changes in your collection to every attached client.</p>

<p>Out of the box Meteor provides you with a mongodb backed reactive collection. While this gives you something very powerful and useful, it is also very inflexible. Each collection is backed by a table in mongodb, and every change to your collection must go through mongo before being broadcast to users.</p>

<p><img class="centered" src="/static/mongo2.gif" /></p>

<p>Notice how delayed the updates on the right start to get compared to the source on the left when using Mongo (above). We could use the <a href="http://devblog.me/meteor-redis.html">meteor-redis</a> plugin, but this is nodejs afterall, why do we need to go through redis when javascript has this built in key value store called object.</p>

<p>To get a quick and dirty object based reactive collection going, we need to satisfy the basic functionality that Meteor expects from a collection.</p>

<pre><code>var isCursor = function (c) {
  return c &amp;&amp; c._publishCursor;
};



https://github.com/meteor/meteor/blob/53cc021064a1dabc02ea811e2a8c2d9977d34c4a/packages/mongo/collection.js
Mongo.Collection._publishCursor = function (cursor, sub, collection) {
  var observeHandle = cursor.observeChanges({
    added: function (id, fields) {
      sub.added(collection, id, fields);
    },
    changed: function (id, fields) {
      sub.changed(collection, id, fields);
    },
    removed: function (id) {
      sub.removed(collection, id);
    }
  });
...
};
</code></pre>

<p>The meteor documentation details the cursor and collection interface, but I’ve found the minimal implmentation looks like</p>

<pre><code>Cursor={forEach, map, fetch, count, observeChanges, _publishCursor}
Collection={insert, update, upsert, find}
</code></pre>

<p>Now this is just a minimal implementation of a meteor collection. There is more that needs to be done to integrate it into Meteor so it works correctly with client side subscriptions, but I’ve found two shortcuts to doing so. One is to monkey patch a dummy meteor collection, letting it setup the routes and connection, and just replacing the internal mongo collection with our own.</p>

<pre><code>//both
Players = new Meteor.Collection('players');

//server
Meteor.startup(function () {
  Players._collection = _Players;
});

Meteor.publish("allplayers", function () {
  return Players.find();
});

//client
Meteor.subscribe('allplayers');
</code></pre>

<p><img class="centered" src="/static/array2.gif" /></p>

<p>Compare the result with the original.</p>

<p>Of course this is just a toy example, but now that we have dispelled some of the meteor magics, you can use it as a starting point for your own custom reactive collections. One simple example could be a collection which immediately processes all updates in memory, but debounces them before saving to mongo. Now you can keep the speedup without losing the  persistence of a mongo backed collection.</p>

</article>


<article class="post">
<h1 class="post-title">Using dnxcore crossplatform</h1>
<time datetime="2015-09-21T00:00:00-04:00" class="post-date">21 Sep 2015</time>
  <p>While I have been running c# applications on linux under mono for quite some time, I have been following the progress of asp.net 5 and waiting for the right time to try the new coreclr. Although the samples on Microsoft’s github are quite simple and it can be confusing to make the leap to a more complicated application, I was able to find a comfortable workflow with some trial and error.</p>

<p>the coreclr works cross platform by allowing you to choose which version to run on, based on the operating system and architecture. You are able to manage this through a command line tool called  dot net version manager (dnvm). This tool simply downloads the appropriate runtime and then configures some enviroment variables which determine what dnx executable will run.</p>

<p>There are currently two versions of dnvm, one is written in bash and the other in powershell. While the docs on the dnvm github recommend using powershell on windows, I prefer to do my work in cygwin. Unfortunately it did not work out of the box on windows for me, but I was able to add windows support and made a pull request. I will have to do some more work before it is accepted, but you may find my version at https://raw.githubusercontent.com/slofurno/Home/dev/dnvm.sh. To run it just type source dnvm.sh after opening your shell, or add it to one of your rc files.</p>

<p>with dnvm installed you can grab the latest dnx by running</p>

<pre><code>dnvm install -r coreclr -arch x64 latest -OS win -u
</code></pre>

<p>The other significant change is how you will be mananging your project dependencies. The runtime is now broken up into many small assemblies, which are all hosted on nuget. Configuration is done with a project.json file, and you are responsible for including any dependencies your project needs for each runtime target you wish to compile for. Again microsoft provides some examples on their github, or you may check out one of mine here https://github.com/slofurno/fsharp-playground/blob/master/fplayground/project.json.</p>

<p>You can set both global dependencies, or per individual target framework. If you have any local dependencies, such as a class library you have created yourself, you must make it available on nuget or add your own nuget package source to the project’s nuget.config. The runtime then selects the correct assembly to use based on convention of the directory.</p>

<p>packaging a class library for multiple target runtimes can be an simple as putting together the project.json, running dnu.cmd restore and dnu.cmd build [–configuration release], and finally putting together a nuget spec file and running nuget pack on it. The nuget spec has some metadata, most critically the package name and version number which you must later reference in any project you want to use it from. It also has references to the assemblies which are included in the package, for example:</p>

</article>


<article class="post">
<h1 class="post-title">writing go with csharp</h1>
<time datetime="2015-09-10T00:00:00-04:00" class="post-date">10 Sep 2015</time>
  <p>It has been awhile since I’ve found something to write about. For a long time I was hoping to write an implementation for techempower’s performance benchmark, as c# is <a href="https://www.techempower.com/benchmarks/#section=data-r10&amp;hw=ec2&amp;test=json">very poorly represented there.</a> For example, on a 2 core ec2 instance, the only c# framework which ran produced 169 responses per second, a tiny fraction of the 86,480 of the top performer. From experience I know that even running a web api on top of a default httplistener could produce at least 10k json responses a second on similiar hardware, and I hoped to gain even greater throughput by dropping down into lower level but higher performance apis and using socketasynceventargs to try and minimize the stress on the gc.</p>

<p>Long story short, the performance benefits were marginal, and I was running into serious issues on mono. While everything worked fine on windows server, when running on a linux machine under mono it seemed that socket objects were not being properly reused. The webserver would leak filedescriptors, and after serving 50,000 requests it would throw an exception. This is not the first inconsistency I have found between mono and the clr on windows. In the past I have found subtle issues between some lesser user functionality such as memory mapped files.</p>

<p>At some point I may write more about the awesome tempest tracker I created for this past season of path of exile, which was finished but never released to the public. Built in react with a blazing fast go backend, the site was designed to push realtime updates to a large concurrent userbase.</p>

<p>One of the benefits of using React is that you don’t have to think much about the state of your ui. Anytime your viewmodel is updated, React will re-render its virtual dom and then calculate which real dom elements need to be updated. This seemed to work great out of the box until I started stress testing the site with hundreds or thousands of updates a second. This was to simluate high user activity, reporting tempests, up or downvoting, adding comments, all of which had to be updated in real time. Its here that React fell over with the sheer quantity of updates. While I solved this by batching updates that were pushed from the server, it felt a little ugly to essentially tell React to re-dif its virtual dom at a pre-defined fps.</p>

<p>More recently I was sitting around talking to some friends about ideas for some multipler game to spend time on between doing other things, and someone floated the idea for a minesweeper game. In typical hackathon style I had a playable game the following day, but since then I have spent some time thinking about how I could refactor both the client and server.</p>

<p>I wanted the server, written in c#, to have its parts less coupled. I wanted the flexibility to add some features like multiple games going on at once or a global high score. So far I’ve been solving this by using c#’s BufferBlock. Similiar to channels in go, they allow events or objects to be passed around between different workers. I think this should lend itself to players moving between minesweeper games.</p>

<p>The client I had decided to hack together in a very imperative style using divs to represent the tiles of the minesweeper game. While this code performed very well, and was very simple to update with each state change, I felt like it would be a little fragile when I tried adding some of the auxilary features that people were requesting.</p>

<p>My first attempt at a solution was to start using Mithril to render the scoreboard, but I still remaing skeptical the libray won’t suffer from the same issue that React did when I try to render the game state with it as well. I’ve created html5 games in the past in the traditional mvc using the canvas or webgl to render, so I am interested to see how this scales, since it is definaly much easier creating squares in the dom then in opengl.</p>

</article>


<article class="post">
<h1 class="post-title">profiling golang</h1>
<time datetime="2015-06-18T00:00:00-04:00" class="post-date">18 Jun 2015</time>
  <p>About a month ago I was working on my submission to the third go challenge, building a web application which produces mosaics from uploaded images. Two winners were announced, and my submissioned received a mention as being “very close”, with feedback that while my mosaics were “excellent”, the runtime was “pretty slow”. This is uncharacteristic of my applications, as I am conscious of performance, but I was new to go at the time, and was focusing on learning the language. I knew my runtimes were high (90 seconds), but I did not know how easy it could be to profile my code at a higer resolution with the right tools.</p>

<p><img src="static/gochallenge.jpg" /></p>

<p>After the winners were revealed, the source code of all entries were made public on github. Looking at the winning submissions, I realized that their image processing was much simplier then mine. While this should have been a winning advantage for me, any benefits were far outweighed by the increase on the application’s runtime. The effort I had put into a more robust and correct image matching algorithm paled in comparision to the importance of the overall user experience, where speed and responsiveness are highly rated.</p>

<p>When I was younger I recieved a black and white mosaic of abraham lincoln as a gift. Even though it was only composed of maybe 20 by 40 images, i remember being impressed at how clear the overall picture was, even though the content of the images was all rather uniform. The images were either landscapes or portraits, yet at a distance the details of the images blurred and their relative lightness or darkness was most important.</p>

<p>Thus I began with the idea that the intensity of the underlying images was key for producing a good mosaic. After doing some research, I found that there was a common mistake that many naive, and even professional, image scaling algorithms made, which skewed the intensity of the resulting images. You can <a href="http://www.4p8.com/eric.brasseur/gamma.html">read more about this effect here</a>, which results from averaging colors as if the intensity encoded in the rgb format were on a linear scale, when it is really on a power scale to pack more visible detail into a limited 8 bit space. Simply taking the rgb colors and averaging them is analogous to trying to find the hypotenuse of a triangle with the equation A + B = C.</p>

<p>My tests showed that the go standard library drawing package made this error. My solution was to write my own code to downsample images by first transforming into a linear colorspace, averaging, and then transforming back to srgb. I could tell my mosaics were improved by this, particularly around areas of detail such as facial features, however this now meant doing an expensive math operation on every pixel of every source image in my pre processing stage.</p>

<pre><code>func sRGBtoLinear(s uint8) float64 {

var z float64 = float64(s) / 255
var L float64

if z &gt; 0.04045 {
    L = math.Pow((z+0.055)/(1.055), 2.4)
} else {
    L = z / 12.92
}

return L
}
</code></pre>

<p>Using Dave Cheney’s <a href="https://github.com/pkg/profile">profile package</a>, we find that more then half of our overall runtime (including network operations to download our source images) takes place in this conversion function, specifically 48% of our runtime, or 52 seconds, is spent calling the math.Pow function.</p>

<p>Now that we have established that we have a function containing expensive math calls with only 256 possible inputs which is called millions of times, a possible solution becomes obvious. The new conversion function becomes a lookup and we precompute the results for all possible inputs.</p>

<p>A new profile confirms the improvement, from 62.65s to 3.04s spent in pre-processing.</p>


</article>


<article class="post">
<h1 class="post-title">using firebase as a free rest api</h1>
<time datetime="2015-06-14T00:00:00-04:00" class="post-date">14 Jun 2015</time>
  <p>If you’ve seen alot of movies, there may come a point where you have trouble finding the next good film to watch. Imdb is a great resource for quickly finding information on a specific movie or actor, but not as great at displaying relationships between them. You may know that Denzel Washington is your favorite actor, but not which of his 54 credits are worth your time.</p>

<p>Luckily imdb makes all this information available in text format, which can be parsed, filtered, and transformed into json. Learning from previous work, it was decided to store our movie data somewhere that would not have to be maintained. While not designed to be used as a rest api, Firebase is versatile enough for this purpose. More then just a document store, it provides some sort and search capability which allows us to query without writing our own backend code.</p>

<pre><code>x.firebaseio.com/tevs.json?orderBy=\"$key\"&amp;limitToFirst={count}&amp;startAt=\"{key}\"
</code></pre>

<p>By keying our movies with a url encoded version of their name, this more or less allows us to search by movie title. Again learning from the complexity of previous apps, getting movie recommendations would be a simple two step process. The general flow of our app would be to allow the user to search for a movie they already know, then aggregate all the movies related through the cast and use some heuristic to rank them as suggestions.</p>

<p>In addition to searching by title, it is necessary to easily move both ways across relationships between actors and films by just accessing our firebase api. As such, each actor object would hold a list of movie ids, and each movie would hold a list of cast ids. Because firebase limits us to a single key per object, movie data has to be duplicated and stored a second time in order to allow a second lookup by id. In retrospect, it would have been sufficient to reference movies by string and reuse the earlier dataset. Firebase does store sparse arrays and objects differently than arrays, so there may be some performance benefit with this method.</p>

<p>One of the caveats of android development is that it is not possible to fully control the lifecycle of an app. External factors, for example if the phone is rotated into landscape view, can cause our activity’s process to be destroyed and recreated. Relying on state stored in the activity can easily cause crashes due to null errors, or at best, require users to redo network calls to our api. It is both easier to reason about and more robust to decouple our long running background tasks from our activity process.</p>

<p>Thanks to Otto, an open source message bus from <a href="http://square.github.io/">square</a>, it is simple for the two parts of our application to communicate. Otto communicates with events. To begin our search, a search event is posted from our activity with a property containing the query string. In our service, an annotated subscriber method is called, which handles calling our api and parsing the results. Our service in turn posts a result event, which could be handled by zero, one, or multiple subscribers.</p>

<p>In this case, there will only ever be at most one subscriber, in our activity class. In the case the activty is in a transient state, in between being shut down and recreated, there will be no active subscriber, however our results will not be lost. Otto also provides a producer annotation, which is called immediately whenever a subscriber registers to listen for an event. Whenever our new activity is created, this provider will immediately run, returning the most recent result (which we cached in our service).</p>

<p>Because of the nature of our app, there are many api calls, each returning data which must then be aggregated before returning a result. Because Firebase is primarily a real time streaming service, not a restful api provider, the official android library for firebase was awkward to use for our purpose. Due to this, a basic library for making http calls to firebase in parallel was created, using a similiar api to firebase’s own library.</p>


</article>




    </main>
    
   </div>
   
   <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-68120130-1', 'auto');
  ga('send', 'pageview');

</script>

  </body>
</html>